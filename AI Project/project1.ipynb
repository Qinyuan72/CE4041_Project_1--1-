{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBlGUlanfAnm"
   },
   "source": [
    "# Version  \n",
    "** 1.2 **  \n",
    "1.2 Support for EXPLORE_MODE, allowing for succinct or full logging of data  \n",
    "1.1 Added plot support  \n",
    "1.0 1st release after review.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This is just an increasing number. If you make a change increase the number - major.minor  \n",
    "\n",
    "\n",
    "Develop a program in Python using the Keras Neural Network to implement a classifier fot the NMIST handwritten digits database. It is recommended to use a convolution neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Team  \n",
    "Qinyuan                 20137095  \n",
    "Eamon Moloney           8457077  \n",
    "Ibrahim Saana Aminu     25381993  \n",
    "Des Powell              9513833  \n",
    "Terence Coffey          15223124  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Constants\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     23\u001b[0m NUM_RUNS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------\n",
    "# Required imports\n",
    "# ------------------------------------------------------\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Constants\n",
    "# ------------------------------------------------------\n",
    "NUM_RUNS = 3\n",
    "NUM_CLASSIFICATIONS = 10\n",
    "INPUT_SHAPE = (28, 28, 1)\n",
    "EPOCH_ITERATIONS = 30\n",
    "VERBOSE_FLAG = 1\n",
    "EXPLORE_MODE = 0\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Hyperparameter set of values\n",
    "# ------------------------------------------------------\n",
    "model_id_list = [3]\n",
    "dropout_rate_list = [0.2]\n",
    "batch_size_list = [32]\n",
    "val_split_list = [0.2]\n",
    "learn_rate_list = [0.0001]\n",
    "kernel_size_list = [5]\n",
    "filter_size_list = [15]\n",
    "regularizer_list = [0.000001]\n",
    "seed_value_list = [42]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# globals\n",
    "# ------------------------------------------------------\n",
    "experiment_count = 0\n",
    "training_accuracy = {model_id: [] for model_id in model_id_list}\n",
    "validation_accuracy = {model_id: [] for model_id in model_id_list}\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# not being used - we always run EPOCH_ITERATIONS\n",
    "# ------------------------------------------------------\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4)\n",
    "fit_callbacks = [early_stopping]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# create the specific set of models to be explored\n",
    "# ------------------------------------------------------\n",
    "def create_model(model_id, filter_size, kernel_size, dropout_rate, regularizer, seed_value):\n",
    "    tf.random.set_seed(seed_value)\n",
    "\n",
    "    # Define model structures based on model_id\n",
    "    if model_id == 1:\n",
    "        model = keras.Sequential([\n",
    "            layers.Input(shape=INPUT_SHAPE),\n",
    "            layers.Conv2D(filter_size, kernel_size=(kernel_size, kernel_size), activation=\"relu\"),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Conv2D(filter_size * 2, kernel_size=(kernel_size, kernel_size), activation=\"relu\"),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            layers.Dense(10, activation=\"softmax\")\n",
    "        ], name='Model1')\n",
    "        model_description = \"Conv2D with two Conv layers, two MaxPool, Dropout, and Dense output.\"\n",
    "    \n",
    "    elif model_id == 2:\n",
    "        model = keras.Sequential([\n",
    "            layers.Input(shape=INPUT_SHAPE),\n",
    "            layers.Conv2D(filter_size, kernel_size=(kernel_size, kernel_size), activation=\"relu\"),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Dropout(dropout_rate), \n",
    "            layers.Conv2D(filter_size * 2, kernel_size=(kernel_size, kernel_size), activation=\"relu\"),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Dropout(dropout_rate), \n",
    "            layers.Flatten(),\n",
    "            layers.Dense(filter_size * 4, activation=\"relu\"),\n",
    "            layers.Dense(10, activation=\"softmax\")\n",
    "        ], name='Model2')\n",
    "        model_description = \"Conv2D with two of Conv,MaxPool,Dropout layers, with one internal Dense layer followed by Dense output.\"\n",
    "\n",
    "    elif model_id == 3:\n",
    "        model = keras.Sequential([\n",
    "            layers.Input(shape=INPUT_SHAPE),\n",
    "            layers.Conv2D(filter_size, kernel_size=(kernel_size, kernel_size), padding='same', activation=\"relu\", \n",
    "                          kernel_initializer=HeNormal(seed=seed_value)),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Conv2D(filter_size, kernel_size=(kernel_size, kernel_size), padding='same', activation=\"relu\", \n",
    "                          kernel_initializer=HeNormal(seed=seed_value), \n",
    "                          kernel_regularizer=regularizers.l2(regularizer)),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(regularizer)),   \n",
    "            layers.Dropout(dropout_rate),\n",
    "            layers.Dense(10, activation=\"softmax\")\n",
    "        ], name='Model3')\n",
    "        model_description = \"Structure based on Lecture notes detail\"\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid model_id. Please use 1, 2, or 3.\")\n",
    "    \n",
    "    return model, model_description\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Load and preprocess dataset\n",
    "# ------------------------------------------------------\n",
    "def load_and_preprocess_data():\n",
    "\n",
    "    # Load and pre-process MNIST dataset\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "    x_train = x_train.astype(\"float32\") / 255.0\n",
    "    x_test = x_test.astype(\"float32\") / 255.0\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes=NUM_CLASSIFICATIONS)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes=NUM_CLASSIFICATIONS)\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Compile and train the model\n",
    "# ------------------------------------------------------\n",
    "def compile_and_train_model(model, x_train, y_train, batch_size, learn_rate, val_split):\n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=Adam(learning_rate=learn_rate),\n",
    "        metrics=['accuracy', Precision(), Recall()]\n",
    "    )\n",
    "    # train the model\n",
    "    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=EPOCH_ITERATIONS, validation_split=val_split,shuffle=True,verbose=VERBOSE_FLAG) #callbacks=fit_callbacks)     \n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Plot Training/Validation Accuracy \n",
    "# ------------------------------------------------------\n",
    "def plot_training_metrics():\n",
    "    nrows = len(model_id_list)\n",
    "    ncols = NUM_RUNS\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 5, nrows * 4))\n",
    "    fig.suptitle('Train and Validation Accuracy Across Models and Runs', fontweight='bold')\n",
    "    colors = plt.cm.tab10.colors\n",
    "\n",
    "    for row, model_id in enumerate(model_id_list):\n",
    "        # Check if each model has collected data for NUM_RUNS\n",
    "        if len(training_accuracy[model_id]) < NUM_RUNS:\n",
    "            print(f\"Warning: Model {model_id} has incomplete data for runs. Expected {NUM_RUNS}, got {len(training_accuracy[model_id])}\")\n",
    "            continue\n",
    "        \n",
    "        for col in range(NUM_RUNS):\n",
    "            ax = axes[row, col] if nrows > 1 else axes[col]\n",
    "            try:\n",
    "                acc = training_accuracy[model_id][col]\n",
    "                val_acc = validation_accuracy[model_id][col]\n",
    "                color = colors[col % len(colors)]\n",
    "                \n",
    "                ax.plot(acc, color=color, linestyle='-', label='Train Accuracy')\n",
    "                ax.plot(val_acc, color=color, linestyle='--', label='Val Accuracy')\n",
    "                ax.set_title(f'Model {model_id} - Run {col + 1}')\n",
    "                ax.set_xlabel('Epoch')\n",
    "                ax.set_ylabel('Accuracy')\n",
    "                ax.legend(loc='best')\n",
    "            except IndexError:\n",
    "                print(f\"Error: Model {model_id} run {col + 1} data is missing.\")\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Run experiments\n",
    "# ------------------------------------------------------\n",
    "def run_experiments():\n",
    "    (x_train, y_train), (x_test, y_test) = load_and_preprocess_data()\n",
    "    \n",
    "    global experiment_count\n",
    "    for params in itertools.product(model_id_list,dropout_rate_list, batch_size_list, val_split_list, learn_rate_list, kernel_size_list, filter_size_list, regularizer_list, seed_value_list):\n",
    "        model_id, dropout_rate, batch_size, val_split, learn_rate, kernel_size, filter_size, regularizer, seed_value = params  \n",
    "        \n",
    "        global training_accuracy, validation_accuracy\n",
    "        experiment_count+=1\n",
    "        \n",
    "        # Create model with model description\n",
    "        model, model_description = create_model(model_id,filter_size, kernel_size, dropout_rate, regularizer, seed_value)\n",
    "        \n",
    "        # Train and evaluate\n",
    "        history = compile_and_train_model(model, x_train, y_train, batch_size, learn_rate, val_split)         \n",
    "              \n",
    "        # predict using the test data\n",
    "        predictions = model.predict(x_test)\n",
    "        predicted_labels = np.argmax(predictions, axis=1)\n",
    "        true_labels = np.argmax(y_test, axis=1)\n",
    "        conf_matrix = confusion_matrix(true_labels, predicted_labels)    \n",
    "        \n",
    "        # evaluate \n",
    "        results_model = model.evaluate(x_test, y_test, verbose=VERBOSE_FLAG)\n",
    "        \n",
    "        # log the data - just the accuracy detail for final delivery\n",
    "        num_epochs_run = early_stopping.stopped_epoch if early_stopping.stopped_epoch > 0 else EPOCH_ITERATIONS\n",
    "        log_header = ((f\"#Epochs: {num_epochs_run}, Dropout Rate: {dropout_rate}, Batch Size: {batch_size}, Validation Split: {val_split}, \"\n",
    "            f\"Learning Rate: {learn_rate}, Kernel Size: {kernel_size}, Filter Size: {filter_size}, Regularizer: {regularizer}, Seed: {seed_value}, \"\n",
    "            f\"Description: {model_description}, \"))\n",
    "        log_data = ((f\"Experiment #{experiment_count}, ACCURACY: Test Accuracy {results_model[1]:.4f}, Training Accuracy {history.history['accuracy'][-1]:.4f}, Validation Accuracy {history.history['val_accuracy'][-1]:.4f}, \"\n",
    "            f\"(Test Loss {results_model[0]:.4f}, Test Precision {results_model[2]:.4f}, Test Recall {results_model[3]:.4f})\"))\n",
    "        data_to_log = (f\"{model.name}\" + \": \" + log_data) if EXPLORE_MODE == 0 else (f\"{model.name}\" + \": \" + log_header + log_data)\n",
    "        print(data_to_log)\n",
    "        \n",
    "        # print out cm and 'model summary' detail\n",
    "        print(f'Confusion Matrix\\n{conf_matrix}\\nModel Summary\\n{model.summary()}')\n",
    "        \n",
    "        # gather accuracy numbers from plotting later\n",
    "        training_accuracy[model_id].append(history.history['accuracy'])\n",
    "        validation_accuracy[model_id].append(history.history['val_accuracy'])\n",
    "        \n",
    "               \n",
    "                    \n",
    "# ------------------------------------------------------\n",
    "# main\n",
    "# ------------------------------------------------------\n",
    "# go for it...\n",
    "experiment_count=0 # incremental counter that uniquely identifies the experiment\n",
    "for number_of_runs in range(NUM_RUNS):\n",
    "    start_time = time.time()\n",
    "    print(f'Start Run #{number_of_runs+1} of {NUM_RUNS}')\n",
    "    run_experiments()\n",
    "    end_time = time.time()\n",
    "    print(f'Finished Run #{number_of_runs+1} of {NUM_RUNS}, duration {(end_time-start_time):.4f} seconds')\n",
    "\n",
    "# plot results from training\n",
    "plot_training_metrics()\n",
    "\n",
    "# the end\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRNNFokbIXa4"
   },
   "source": [
    "Application using Keras for MNIST Digit Classifier using 2 convolution layers , 2 drop out layers and 2 accumulation layers. Split the dataset into training and test data in the ratio 70 percent and 30 percent respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPxvrZuYrdCC"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqiWApzUI8hk"
   },
   "source": [
    "Display accuracy of Testing and Validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXcGog1PJNbA"
   },
   "source": [
    "Discuss the Project especially\n",
    "- How you choose to tackle it\n",
    "- What design decisions you made\n",
    "- What the results are like\n",
    "- What you might do better/differently next time you had to tackle a similar project\n",
    "- If plots are called for they should be in your code and in your report.\n",
    "- Marks for neat well designed code with appropriate level of comments\n",
    "- neat logically laid out and informative reports.\n",
    "- Provide classification accuracy for the training and test data. The test data should be split in the ration 70 to 80 and the baance for validation.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
